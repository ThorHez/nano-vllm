#!/usr/bin/env python3
"""
æµå¼è¾“å‡ºåŠŸèƒ½æµ‹è¯•è„šæœ¬

å¿«é€Ÿæµ‹è¯•æ–°æ·»åŠ çš„æµå¼ç”ŸæˆåŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import sys
import time
from nanovllm.llm import LLM
from nanovllm.sampling_params import SamplingParams


def test_basic_streaming():
    """æµ‹è¯•åŸºæœ¬æµå¼ç”ŸæˆåŠŸèƒ½"""
    print("æµ‹è¯•åŸºæœ¬æµå¼ç”Ÿæˆ...")
    
    try:
        llm = LLM("../models/Qwen3-0.6B")
        
        prompt = "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹è¯­è¨€ã€‚"
        print(f"æç¤º: {prompt}")
        print("ç”Ÿæˆ: ", end='', flush=True)
        
        token_count = 0
        start_time = time.time()
        
        for chunk in llm.stream(prompt, temperature=0.8, max_tokens=50):
            if not chunk['finished']:
                print(chunk['text'], end='', flush=True)
                token_count += 1
            else:
                end_time = time.time()
                duration = end_time - start_time
                speed = token_count / duration if duration > 0 else 0
                print(f"\nâœ… åŸºæœ¬æµå¼ç”Ÿæˆæµ‹è¯•é€šè¿‡")
                print(f"   ç”Ÿæˆäº† {token_count} ä¸ªtokenï¼Œé€Ÿåº¦: {speed:.1f} tok/s")
                return True
                
    except Exception as e:
        print(f"\nâŒ åŸºæœ¬æµå¼ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        return False


def test_chat_streaming():
    """æµ‹è¯•èŠå¤©æµå¼ç”ŸæˆåŠŸèƒ½"""
    print("\næµ‹è¯•èŠå¤©æµå¼ç”Ÿæˆ...")
    
    try:
        llm = LLM("../models/Qwen3-0.6B")
        
        prompt = "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
        print(f"ç”¨æˆ·é—®é¢˜: {prompt}")
        print("AIå›ç­”: ", end='', flush=True)
        
        response = llm.chat_stream(prompt, temperature=0.7, max_tokens=60)
        
        print(f"âœ… èŠå¤©æµå¼ç”Ÿæˆæµ‹è¯•é€šè¿‡")
        print(f"   å®Œæ•´å›ç­”é•¿åº¦: {len(response)} å­—ç¬¦")
        return True
        
    except Exception as e:
        print(f"\nâŒ èŠå¤©æµå¼ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        return False


def test_batch_streaming():
    """æµ‹è¯•æ‰¹é‡æµå¼ç”ŸæˆåŠŸèƒ½"""
    print("\næµ‹è¯•æ‰¹é‡æµå¼ç”Ÿæˆ...")
    
    try:
        llm = LLM("../models/Qwen3-0.6B")
        
        prompts = [
            "ä»Šå¤©å¤©æ°”å¦‚ä½•ï¼Ÿ",
            "æ¨èä¸€æœ¬ä¹¦ã€‚"
        ]
        
        print("æ‰¹é‡æç¤º:")
        for i, p in enumerate(prompts, 1):
            print(f"  {i}. {p}")
        
        print("\næ‰¹é‡ç”Ÿæˆç»“æœ:")
        
        seq_responses = {}
        for batch_output in llm.stream_batch(prompts, temperature=0.8, max_tokens=30):
            for seq_id, chunk in batch_output.items():
                if seq_id not in seq_responses:
                    seq_responses[seq_id] = ""
                    print(f"åºåˆ—{seq_id}: ", end='', flush=True)
                
                if not chunk['finished']:
                    print(chunk['text'], end='', flush=True)
                    seq_responses[seq_id] += chunk['text']
                else:
                    print(f" [å®Œæˆ]")
        
        print(f"âœ… æ‰¹é‡æµå¼ç”Ÿæˆæµ‹è¯•é€šè¿‡")
        print(f"   å¤„ç†äº† {len(seq_responses)} ä¸ªåºåˆ—")
        return True
        
    except Exception as e:
        print(f"\nâŒ æ‰¹é‡æµå¼ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        return False


def test_low_level_api():
    """æµ‹è¯•åº•å±‚API"""
    print("\næµ‹è¯•åº•å±‚æµå¼API...")
    
    try:
        llm = LLM("../models/Qwen3-0.6B")
        
        sampling_params = SamplingParams(
            temperature=0.9,
            max_tokens=40,
            ignore_eos=False
        )
        
        prompt = "äººå·¥æ™ºèƒ½çš„å®šä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ"
        print(f"æç¤º: {prompt}")
        print("ç”Ÿæˆ: ", end='', flush=True)
        
        for chunk in llm.generate_stream(prompt, sampling_params):
            if not chunk['finished']:
                print(chunk['text'], end='', flush=True)
            else:
                print(f"\nâœ… åº•å±‚APIæµ‹è¯•é€šè¿‡")
                print(f"   æœ€ç»ˆæ–‡æœ¬é•¿åº¦: {len(chunk['total_text'])} å­—ç¬¦")
                print(f"   Tokenæ•°é‡: {len(chunk['total_tokens'])}")
                return True
                
    except Exception as e:
        print(f"\nâŒ åº•å±‚APIæµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("=" * 60)
    print("nanovllm æµå¼è¾“å‡ºåŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    tests = [
        test_basic_streaming,
        test_chat_streaming,
        test_batch_streaming,
        test_low_level_api,
    ]
    
    passed = 0
    total = len(tests)
    
    for test_func in tests:
        try:
            if test_func():
                passed += 1
        except KeyboardInterrupt:
            print("\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
            break
        except Exception as e:
            print(f"æµ‹è¯•æ‰§è¡Œå‡ºé”™: {e}")
    
    print("\n" + "=" * 60)
    print(f"æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµå¼è¾“å‡ºåŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
        return 0
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°")
        return 1


if __name__ == "__main__":
    sys.exit(main()) 